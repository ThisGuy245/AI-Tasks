{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvgwb8tVu42DBHtC7w7xtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThisGuy245/AI-Tasks/blob/main/AI_Task_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK B - Large Language Model**\n",
        "\n",
        "Start with installs"
      ],
      "metadata": {
        "id": "zBp5S8uXLs0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install onnxruntime\n"
      ],
      "metadata": {
        "id": "DE5K3kzXLrcX",
        "outputId": "d45d3925-e52a-49b6-a0de-072ab5f5e8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.12.23)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zgTiscI2-sWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f326765f-6c39-4eed-bb7c-abb221b114b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-9768b0c78325>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9768b0c78325>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    huggingface-cli login\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import time\n",
        "import difflib\n",
        "\n",
        "# Login to Hugging Face CLI (ensure you have your token ready)\n",
        "huggingface-cli login\n",
        "\n",
        "# Load the model and tokenizer with 8-bit quantization for memory efficiency\n",
        "model_name = \"facebook/llama-3.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Function to generate a fluent response with faster response times\n",
        "def generate_response(prompt, max_new_tokens=60, temperature=0.7, top_k=40):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    start_time = time.time()\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        no_repeat_ngram_size=2,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    response_time = end_time - start_time\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    ai_response = response.split(\"AI:\")[-1].strip()\n",
        "    return ai_response, response_time\n",
        "\n",
        "# Function to handle opening questions\n",
        "def handle_opening_questions(user_input):\n",
        "    opening_phrases = {\n",
        "        \"hello\": \"Hello! I am an AI who talks about more of my kind! Ask me questions about AI.\",\n",
        "        \"hi\": \"Hi there! I'm an AI focused on AI. Feel free to ask me anything about AI advancements.\",\n",
        "        \"who are you\": \"I am an AI who specializes in AI! Ask me questions about AI and its developments.\",\n",
        "        \"what do you do\": \"I talk about AI, its progress, and its impact. Ask me anything related to AI!\",\n",
        "        \"how are you\": \"I'm an AI, so I don't have feelings, but thanks for asking! Ask me about AI advancements!\"\n",
        "    }\n",
        "\n",
        "    user_input = user_input.lower()\n",
        "    for key, response in opening_phrases.items():\n",
        "        if key in user_input:\n",
        "            return response\n",
        "    return None\n",
        "\n",
        "# Function to check if input contains relevant AI-related keywords\n",
        "def is_relevant_to_ai(user_input):\n",
        "    ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', 'neural network', 'language model', 'robotics', 'natural language processing', 'computer vision', 'reinforcement learning', 'data science']\n",
        "\n",
        "    # Check if any AI-related keyword is in the user's input\n",
        "    return any(keyword in user_input.lower() for keyword in ai_keywords)\n",
        "\n",
        "# Function to dynamically adjust context and retain important info\n",
        "def update_context(user_input, ai_response, conversation_history, max_context_length=1000):\n",
        "    important_info = extract_important_info(user_input)\n",
        "    conversation_history += f\"\\nUser: {user_input}\\nAI: {ai_response}\\n\"\n",
        "\n",
        "    # Retain key information for future context (you can refine the extraction logic)\n",
        "    if len(conversation_history) > max_context_length:\n",
        "        conversation_history = conversation_history[-max_context_length:]\n",
        "\n",
        "    return conversation_history, important_info\n",
        "\n",
        "# Function to extract important information from the user's input (simplified for example)\n",
        "def extract_important_info(user_input):\n",
        "    # Here, you can add more complex logic for extracting important context (e.g., named entities)\n",
        "    if \"name\" in user_input:\n",
        "        return \"user_name\"\n",
        "    return \"\"\n",
        "\n",
        "# FAQ for more detailed responses\n",
        "faqs = {\n",
        "    \"What is AI?\": \"Artificial Intelligence (AI) refers to computer systems designed to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\",\n",
        "\n",
        "    \"How does machine learning work?\": \"Machine learning is a subset of AI that focuses on algorithms that can learn from and make predictions or decisions based on data. It involves training models on large datasets to recognize patterns and make informed decisions.\",\n",
        "\n",
        "    \"What are the latest advancements in AI?\": \"Recent AI advancements include improvements in natural language processing, computer vision, reinforcement learning, and generative AI. Specific examples include GPT-3 for language tasks, AlphaFold for protein structure prediction, and DALL-E for image generation.\",\n",
        "\n",
        "    \"What are the ethical concerns surrounding AI?\": \"Key ethical concerns in AI include bias in algorithms, privacy issues, job displacement, autonomous weapons, and the potential for AI to be used for surveillance or manipulation. Ensuring responsible AI development is a major focus in the field.\",\n",
        "\n",
        "    \"How is AI impacting different industries?\": \"AI is transforming industries like healthcare (diagnosis and drug discovery), finance (fraud detection and algorithmic trading), transportation (autonomous vehicles), and entertainment (personalized content recommendations).\",\n",
        "\n",
        "    \"What is the future of AI?\": \"The future of AI may include more advanced general AI systems, improved human-AI collaboration, breakthroughs in quantum AI, and potentially the development of artificial general intelligence (AGI).\",\n",
        "}\n",
        "\n",
        "# Function to check if user input matches any FAQ or is similar enough to be treated as a match\n",
        "def find_similar_faq(user_input):\n",
        "    # Find the FAQ question that most closely matches the user input\n",
        "    highest_similarity = 0\n",
        "    best_match = None\n",
        "\n",
        "    # Use difflib's SequenceMatcher for finding similar strings\n",
        "    for question in faqs:\n",
        "        similarity = difflib.SequenceMatcher(None, user_input.lower(), question.lower()).ratio()\n",
        "        if similarity > highest_similarity:\n",
        "            highest_similarity = similarity\n",
        "            best_match = question\n",
        "\n",
        "    # If the similarity is above a certain threshold (e.g., 0.8), return the corresponding FAQ answer\n",
        "    if highest_similarity >= 0.8:\n",
        "        return faqs[best_match]\n",
        "    return None\n",
        "\n",
        "# Main conversation loop\n",
        "context = \"You are an AI assistant specializing in discussing AI advancements. Provide informative and coherent responses about AI progress, its impact on society, and potential future developments.\"\n",
        "conversation_history = \"\"\n",
        "\n",
        "print(\"\\n\\n\\nWelcome to the AI Chat! Let's discuss AI advancements.\")\n",
        "print(\"You can start chatting. Type 'exit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Handle common opening questions\n",
        "    opening_response = handle_opening_questions(user_input)\n",
        "    if opening_response:\n",
        "        print(f\"AI: {opening_response}\")\n",
        "        print(\"Response time: 0.00 seconds (Opening Question)\")\n",
        "        conversation_history += f\"\\nUser: {user_input}\\nAI: {opening_response}\\n\"\n",
        "    else:\n",
        "        # First check for similar FAQ questions\n",
        "        faq_response = find_similar_faq(user_input)\n",
        "        if faq_response:\n",
        "            print(f\"AI: {faq_response}\")\n",
        "            print(\"Response time: 0.00 seconds (FAQ)\")\n",
        "            conversation_history += f\"\\nUser: {user_input}\\nAI: {faq_response}\\n\"\n",
        "        elif is_relevant_to_ai(user_input):\n",
        "            # Generate AI response if input is AI-related\n",
        "            prompt = f\"{context}\\n{conversation_history}\\nUser: {user_input}\\nAI:\"\n",
        "            ai_response, response_time = generate_response(prompt, max_new_tokens=60, temperature=0.7, top_k=40)\n",
        "\n",
        "            print(f\"AI: {ai_response}\")\n",
        "            print(f\"Response time: {response_time:.2f} seconds\")\n",
        "            conversation_history, important_info = update_context(user_input, ai_response, conversation_history)\n",
        "        else:\n",
        "            # If the input is not relevant to AI, ask to stay on topic\n",
        "            print(\"AI: I like to stay on topic... Let's talk about AI!\")\n",
        "            conversation_history += f\"\\nUser: {user_input}\\nAI: I like to stay on topic... Let's talk about AI!\\n\"\n",
        "\n",
        "    # Optionally, log or process the retained important information\n",
        "    if important_info:\n",
        "        print(f\"Retained information: {important_info}\")\n",
        "\n",
        "print(\"Thank you for chatting! Here's a summary of our conversation:\")\n",
        "print(conversation_history)\n"
      ]
    }
  ]
}