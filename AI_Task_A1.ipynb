{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5ZQ71zW2640kQRcy8VGR6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThisGuy245/AI-Tasks/blob/main/AI_Task_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Assignment, Task A1\n"
      ],
      "metadata": {
        "id": "WO6HGNNIVJ_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NPC's Power |\tEnemy's Power\t| Decision/Action\n",
        "\n",
        "---\n",
        "\n",
        "Strong (1)\t|  Strong (1)\t|  Attack (0)    >>   I should predict 0\n",
        "\n",
        "---\n",
        "\n",
        "Weak (0)\t|  Strong (1)\t |  Flee (1)    >>   I should predict 1\n",
        "\n",
        "---\n",
        "\n",
        "Strong (1)\t|  Weak (0)\t|  Flee (1)    >>    I should predict 1\n",
        "\n",
        "---\n",
        "\n",
        "Weak (0)\t|  Weak (0)\t|  Attack (0)    >>    I should predict 0\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oE7WyM_tVad8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Input data: NPC power vs. Enemy power / Créer une gamme pour les données d'entrer (pouvoir PNJ contre. pouvoir enemie)\n",
        "x = np.array([[1, 1], [0, 1], [1, 0], [0, 0]], dtype=np.float32)\n",
        "\n",
        "# Target data: Decision/Action (0 = attack, 1 = flee) / Créer une gamme pour les données ciblé (Decision/Action)\n",
        "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors / Converté les gammes NumPy a des tensors PyTorch\n",
        "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Neural Network Model / Modèle de réseau neuronal\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, activation=nn.ReLU):  # Default to ReLU / Defaut a ReLu\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.hidden = nn.Linear(2, 3)  # Input layer to hidden layer / Calque entrée a Calque caché\n",
        "        self.output = nn.Linear(3, 1)  # Hidden layer to output layer / Calque caché a Calque sortie\n",
        "        self.activation = activation()  # Activation function (modular) / Fonction d'Activation (modulére)\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden(x))  # Apply selected activation function\n",
        "        x = self.sigmoid(self.output(x))  # Apply Sigmoid to output layer\n",
        "        return x\n",
        "\n",
        "# Define learning rates to test\n",
        "learning_rates = [0.001, 0.01, 0.1]  # TALK ABOUT DIFFERENCE MAKE SURE TO START WITH 0.1 THEN GO SMALLER\n",
        "\n",
        "# Define activation functions to test\n",
        "activations = [nn.ReLU, nn.Sigmoid, nn.Tanh]  # TALK ABOUT TANH WORKING AND/OR ReLU FOR HIDDEN AND SIGMOID FOR OUTPUT\n",
        "\n",
        "# Loop through each activation function\n",
        "for activation in activations:\n",
        "    print(f\"\\nTesting Activation Function: {activation.__name__}\")\n",
        "\n",
        "    # Initialize the model with the current activation function\n",
        "    model = NeuralNetwork(activation=activation)\n",
        "\n",
        "    # Loop through each learning rate / Loopé entre chaque LR\n",
        "    for lr in learning_rates:\n",
        "        print(f\"  Testing Learning Rate: {lr}\")\n",
        "\n",
        "        # Define the optimizer / Definé l'optimiseur\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "        # Define the loss function\n",
        "        criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "\n",
        "        # Training loop / Loop d'entrainement\n",
        "        epochs = 5000\n",
        "        for epoch in range(epochs):\n",
        "            outputs = model(x_tensor)\n",
        "            loss = criterion(outputs, y_tensor)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print loss every 500 epochs / Imprimer les perds a chaque 500 epochs\n",
        "            if (epoch + 1) % 500 == 0:\n",
        "                print(f\"    Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Validate the model after training\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x_tensor)\n",
        "            predictions = torch.round(predictions)\n",
        "            print(\"    Predictions:\", predictions.numpy())\n",
        "            print(\"    Actual:     \", y_tensor.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhzAfPYVJkb",
        "outputId": "ef9592cc-b689-4497-fb4a-3682342215a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Activation Function: ReLU\n",
            "  Testing Learning Rate: 0.001\n",
            "    Epoch [500/5000], Loss: 0.7002\n",
            "    Epoch [1000/5000], Loss: 0.6975\n",
            "    Epoch [1500/5000], Loss: 0.6959\n",
            "    Epoch [2000/5000], Loss: 0.6949\n",
            "    Epoch [2500/5000], Loss: 0.6943\n",
            "    Epoch [3000/5000], Loss: 0.6940\n",
            "    Epoch [3500/5000], Loss: 0.6938\n",
            "    Epoch [4000/5000], Loss: 0.6936\n",
            "    Epoch [4500/5000], Loss: 0.6935\n",
            "    Epoch [5000/5000], Loss: 0.6935\n",
            "    Predictions: [[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "  Testing Learning Rate: 0.01\n",
            "    Epoch [500/5000], Loss: 0.6933\n",
            "    Epoch [1000/5000], Loss: 0.6933\n",
            "    Epoch [1500/5000], Loss: 0.6932\n",
            "    Epoch [2000/5000], Loss: 0.6932\n",
            "    Epoch [2500/5000], Loss: 0.6932\n",
            "    Epoch [3000/5000], Loss: 0.6932\n",
            "    Epoch [3500/5000], Loss: 0.6932\n",
            "    Epoch [4000/5000], Loss: 0.6932\n",
            "    Epoch [4500/5000], Loss: 0.6932\n",
            "    Epoch [5000/5000], Loss: 0.6932\n",
            "    Predictions: [[1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "  Testing Learning Rate: 0.1\n",
            "    Epoch [500/5000], Loss: 0.6931\n",
            "    Epoch [1000/5000], Loss: 0.6931\n",
            "    Epoch [1500/5000], Loss: 0.6931\n",
            "    Epoch [2000/5000], Loss: 0.6931\n",
            "    Epoch [2500/5000], Loss: 0.6931\n",
            "    Epoch [3000/5000], Loss: 0.6931\n",
            "    Epoch [3500/5000], Loss: 0.6931\n",
            "    Epoch [4000/5000], Loss: 0.6931\n",
            "    Epoch [4500/5000], Loss: 0.6931\n",
            "    Epoch [5000/5000], Loss: 0.6931\n",
            "    Predictions: [[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "\n",
            "Testing Activation Function: Sigmoid\n",
            "  Testing Learning Rate: 0.001\n",
            "    Epoch [500/5000], Loss: 0.6965\n",
            "    Epoch [1000/5000], Loss: 0.6955\n",
            "    Epoch [1500/5000], Loss: 0.6948\n",
            "    Epoch [2000/5000], Loss: 0.6944\n",
            "    Epoch [2500/5000], Loss: 0.6940\n",
            "    Epoch [3000/5000], Loss: 0.6938\n",
            "    Epoch [3500/5000], Loss: 0.6937\n",
            "    Epoch [4000/5000], Loss: 0.6936\n",
            "    Epoch [4500/5000], Loss: 0.6935\n",
            "    Epoch [5000/5000], Loss: 0.6935\n",
            "    Predictions: [[1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "  Testing Learning Rate: 0.01\n",
            "    Epoch [500/5000], Loss: 0.6934\n",
            "    Epoch [1000/5000], Loss: 0.6934\n",
            "    Epoch [1500/5000], Loss: 0.6933\n",
            "    Epoch [2000/5000], Loss: 0.6933\n",
            "    Epoch [2500/5000], Loss: 0.6933\n",
            "    Epoch [3000/5000], Loss: 0.6933\n",
            "    Epoch [3500/5000], Loss: 0.6933\n",
            "    Epoch [4000/5000], Loss: 0.6933\n",
            "    Epoch [4500/5000], Loss: 0.6933\n",
            "    Epoch [5000/5000], Loss: 0.6933\n",
            "    Predictions: [[1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "  Testing Learning Rate: 0.1\n",
            "    Epoch [500/5000], Loss: 0.6932\n",
            "    Epoch [1000/5000], Loss: 0.6931\n",
            "    Epoch [1500/5000], Loss: 0.6930\n",
            "    Epoch [2000/5000], Loss: 0.6928\n",
            "    Epoch [2500/5000], Loss: 0.6925\n",
            "    Epoch [3000/5000], Loss: 0.6919\n",
            "    Epoch [3500/5000], Loss: 0.6901\n",
            "    Epoch [4000/5000], Loss: 0.6842\n",
            "    Epoch [4500/5000], Loss: 0.6647\n",
            "    Epoch [5000/5000], Loss: 0.6161\n",
            "    Predictions: [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "\n",
            "Testing Activation Function: Tanh\n",
            "  Testing Learning Rate: 0.001\n",
            "    Epoch [500/5000], Loss: 0.7487\n",
            "    Epoch [1000/5000], Loss: 0.7282\n",
            "    Epoch [1500/5000], Loss: 0.7150\n",
            "    Epoch [2000/5000], Loss: 0.7065\n",
            "    Epoch [2500/5000], Loss: 0.7009\n",
            "    Epoch [3000/5000], Loss: 0.6973\n",
            "    Epoch [3500/5000], Loss: 0.6949\n",
            "    Epoch [4000/5000], Loss: 0.6933\n",
            "    Epoch [4500/5000], Loss: 0.6922\n",
            "    Epoch [5000/5000], Loss: 0.6915\n",
            "    Predictions: [[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "  Testing Learning Rate: 0.01\n",
            "    Epoch [500/5000], Loss: 0.6888\n",
            "    Epoch [1000/5000], Loss: 0.6861\n",
            "    Epoch [1500/5000], Loss: 0.6812\n",
            "    Epoch [2000/5000], Loss: 0.6727\n",
            "    Epoch [2500/5000], Loss: 0.6587\n",
            "    Epoch [3000/5000], Loss: 0.6374\n",
            "    Epoch [3500/5000], Loss: 0.6079\n",
            "    Epoch [4000/5000], Loss: 0.5718\n",
            "    Epoch [4500/5000], Loss: 0.5339\n",
            "    Epoch [5000/5000], Loss: 0.4988\n",
            "    Predictions: [[1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "  Testing Learning Rate: 0.1\n",
            "    Epoch [500/5000], Loss: 0.1289\n",
            "    Epoch [1000/5000], Loss: 0.0412\n",
            "    Epoch [1500/5000], Loss: 0.0234\n",
            "    Epoch [2000/5000], Loss: 0.0162\n",
            "    Epoch [2500/5000], Loss: 0.0123\n",
            "    Epoch [3000/5000], Loss: 0.0099\n",
            "    Epoch [3500/5000], Loss: 0.0083\n",
            "    Epoch [4000/5000], Loss: 0.0072\n",
            "    Epoch [4500/5000], Loss: 0.0063\n",
            "    Epoch [5000/5000], Loss: 0.0056\n",
            "    Predictions: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "    Actual:      [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    }
  ]
}